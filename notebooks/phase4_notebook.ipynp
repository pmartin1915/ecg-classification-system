# Phase 4: Model Training and Evaluation

# %% [markdown]
# # Phase 4: Model Training and Evaluation
# 
# This notebook implements comprehensive model training for ECG classification.
# 
# ## Objectives:
# 1. Load feature data from Phase 3
# 2. Train multiple ML models with optimized parameters
# 3. Handle class imbalance with SMOTE
# 4. Perform hyperparameter tuning
# 5. Evaluate models and select the best performer
# 6. Save trained models for deployment

# %% [markdown]
# ## 1. Setup and Imports

# %%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import pickle
import joblib
import warnings
from datetime import datetime
import gc
import psutil
import os

# Machine Learning
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve, auc
)

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier

# Imbalanced learning
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Visualization
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)

print("‚úÖ Imports completed successfully")

# %% [markdown]
# ## 2. Configuration

# %%
class Config:
    """Configuration for Phase 4"""
    # Data paths
    PHASE3_RESULTS = Path('data/processed/feature_extraction_results.pkl')
    MODEL_OUTPUT_DIR = Path('models/trained_models')
    VISUALIZATION_DIR = Path('visualizations/phase4')
    
    # Data split parameters
    TEST_SIZE = 0.2
    VAL_SIZE = 0.1
    RANDOM_STATE = 42
    
    # Model training parameters
    USE_SMOTE = True
    USE_HYPERPARAMETER_TUNING = True
    USE_CROSS_VALIDATION = True
    CV_FOLDS = 5
    
    # Models to train
    MODEL_KEYS = [
        'logistic_regression',
        'random_forest',
        'gradient_boosting',
        'svm',
        'neural_network',
        'knn'
    ]
    
    # Visualization settings
    CREATE_PLOTS = True
    SAVE_PLOTS = True
    
    # Memory optimization
    CHUNK_SIZE = 5000
    OPTIMIZE_MEMORY = True

# Create directories
Config.MODEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
Config.VISUALIZATION_DIR.mkdir(parents=True, exist_ok=True)

print("‚úÖ Configuration loaded")

# %% [markdown]
# ## 3. Memory and Performance Monitoring

# %%
class PerformanceMonitor:
    """Monitor memory and performance during training"""
    
    def __init__(self):
        self.start_memory = self.get_memory_usage()
        self.start_time = datetime.now()
    
    @staticmethod
    def get_memory_usage():
        """Get current memory usage in MB"""
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024 / 1024
    
    def report(self, stage: str):
        """Report memory and time for a stage"""
        current_memory = self.get_memory_usage()
        memory_used = current_memory - self.start_memory
        time_elapsed = (datetime.now() - self.start_time).total_seconds()
        
        print(f"\nüìä {stage}")
        print(f"   Memory: {current_memory:.1f} MB (Œî {memory_used:+.1f} MB)")
        print(f"   Time: {time_elapsed:.1f}s")
        
        return {
            'stage': stage,
            'memory_mb': current_memory,
            'memory_delta_mb': memory_used,
            'time_seconds': time_elapsed
        }

monitor = PerformanceMonitor()

# %% [markdown]
# ## 4. Load Feature Data from Phase 3

# %%
def load_phase3_data():
    """Load feature data from Phase 3"""
    print("Loading Phase 3 results...")
    
    if not Config.PHASE3_RESULTS.exists():
        # Try alternative path
        alt_path = Path('data/processed/phase3_features.pkl')
        if alt_path.exists():
            Config.PHASE3_RESULTS = alt_path
        else:
            raise FileNotFoundError(
                f"Phase 3 results not found at {Config.PHASE3_RESULTS}\n"
                "Please run Phase 3 first!"
            )
    
    with open(Config.PHASE3_RESULTS, 'rb') as f:
        phase3_data = pickle.load(f)
    
    # Extract data with flexible key handling
    if 'X_features' in phase3_data:
        X = phase3_data['X_features']
        y = phase3_data['y_encoded']
        feature_names = phase3_data['feature_names']
    else:
        X = phase3_data.get('X', phase3_data.get('features'))
        y = phase3_data.get('y', phase3_data.get('labels'))
        feature_names = phase3_data.get('feature_names', 
                                       [f'feature_{i}' for i in range(X.shape[1])])
    
    # Convert to numpy arrays if needed
    X = np.array(X)
    y = np.array(y)
    
    print(f"‚úÖ Loaded data shape: {X.shape}")
    print(f"   Number of samples: {X.shape[0]:,}")
    print(f"   Number of features: {X.shape[1]}")
    print(f"   Number of classes: {len(np.unique(y))}")
    
    return X, y, feature_names

# Load data
try:
    X_features, y_encoded, feature_names = load_phase3_data()
    monitor.report("Data Loading")
except Exception as e:
    print(f"‚ùå Error loading data: {e}")
    print("\nUsing synthetic data for demonstration...")
    
    # Create synthetic data
    np.random.seed(42)
    n_samples = 1000
    n_features = 50
    n_classes = 5
    
    X_features = np.random.randn(n_samples, n_features)
    y_encoded = np.random.randint(0, n_classes, n_samples)
    feature_names = [f'feature_{i}' for i in range(n_features)]

# %% [markdown]
# ## 5. Data Exploration and Class Distribution

# %%
# Class distribution
class_names = ['NORM', 'MI', 'STTC', 'CD', 'HYP']
class_counts = pd.Series(y_encoded).value_counts().sort_index()

# Create visualization
fig = make_subplots(
    rows=1, cols=2,
    subplot_titles=('Class Distribution', 'Class Percentages'),
    specs=[[{'type': 'bar'}, {'type': 'pie'}]]
)

# Bar chart
fig.add_trace(
    go.Bar(
        x=[class_names[i] for i in class_counts.index],
        y=class_counts.values,
        marker_color='lightblue',
        text=class_counts.values,
        textposition='auto'
    ),
    row=1, col=1
)

# Pie chart
fig.add_trace(
    go.Pie(
        labels=[class_names[i] for i in class_counts.index],
        values=class_counts.values,
        hole=0.3
    ),
    row=1, col=2
)

fig.update_layout(
    title_text="ECG Class Distribution",
    showlegend=False,
    height=400
)

fig.show()

# Print class imbalance ratio
print("\nüìä Class Distribution:")
for i, count in enumerate(class_counts.values):
    percentage = (count / len(y_encoded)) * 100
    print(f"   {class_names[i]}: {count:,} samples ({percentage:.1f}%)")

imbalance_ratio = class_counts.max() / class_counts.min()
print(f"\n‚öñÔ∏è  Class imbalance ratio: {imbalance_ratio:.2f}:1")

# %% [markdown]
# ## 6. Data Preparation and Splitting

# %%
def prepare_data_splits(X, y, test_size=0.2, val_size=0.1, random_state=42):
    """Create train/validation/test splits with stratification"""
    
    # First split: train+val vs test
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=test_size, stratify=y, random_state=random_state
    )
    
    # Second split: train vs val
    val_size_adjusted = val_size / (1 - test_size)
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=val_size_adjusted, 
        stratify=y_temp, random_state=random_state
    )
    
    return X_train, X_val, X_test, y_train, y_val, y_test

# Create splits
X_train, X_val, X_test, y_train, y_val, y_test = prepare_data_splits(
    X_features, y_encoded, 
    test_size=Config.TEST_SIZE,
    val_size=Config.VAL_SIZE,
    random_state=Config.RANDOM_STATE
)

print("‚úÖ Data split completed:")
print(f"   Training set: {X_train.shape[0]:,} samples")
print(f"   Validation set: {X_val.shape[0]:,} samples")
print(f"   Test set: {X_test.shape[0]:,} samples")

# Verify stratification
print("\nüìä Class distribution in splits:")
for split_name, y_split in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:
    dist = np.bincount(y_split) / len(y_split) * 100
    print(f"   {split_name}: {' | '.join([f'{class_names[i]}: {d:.1f}%' for i, d in enumerate(dist)])}")

monitor.report("Data Splitting")

# %% [markdown]
# ## 7. Feature Scaling

# %%
# Initialize and fit scaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

print("‚úÖ Feature scaling completed")
print(f"   Mean: {X_train_scaled.mean():.6f}")
print(f"   Std: {X_train_scaled.std():.6f}")

# Save scaler for later use
scaler_path = Config.MODEL_OUTPUT_DIR / 'scaler.pkl'
joblib.dump(scaler, scaler_path)
print(f"   Scaler saved to: {scaler_path}")

# %% [markdown]
# ## 8. Handle Class Imbalance with SMOTE

# %%
if Config.USE_SMOTE:
    print("Applying SMOTE to handle class imbalance...")
    
    # Apply SMOTE
    smote = SMOTE(random_state=Config.RANDOM_STATE, n_jobs=-1)
    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)
    
    print(f"‚úÖ SMOTE applied successfully")
    print(f"   Original training samples: {X_train_scaled.shape[0]:,}")
    print(f"   Balanced training samples: {X_train_balanced.shape[0]:,}")
    
    # Show new class distribution
    balanced_counts = pd.Series(y_train_balanced).value_counts().sort_index()
    print("\nüìä Balanced class distribution:")
    for i, count in enumerate(balanced_counts.values):
        print(f"   {class_names[i]}: {count:,} samples")
else:
    X_train_balanced = X_train_scaled
    y_train_balanced = y_train

monitor.report("Class Balancing")

# %% [markdown]
# ## 9. Model Definition and Hyperparameter Grids

# %%
# Define models
models = {
    'logistic_regression': LogisticRegression(
        max_iter=1000,
        class_weight='balanced',
        random_state=Config.RANDOM_STATE,
        n_jobs=-1
    ),
    'random_forest': RandomForestClassifier(
        n_estimators=100,
        class_weight='balanced',
        random_state=Config.RANDOM_STATE,
        n_jobs=-1
    ),
    'gradient_boosting': GradientBoostingClassifier(
        n_estimators=100,
        random_state=Config.RANDOM_STATE
    ),
    'svm': SVC(
        kernel='rbf',
        class_weight='balanced',
        probability=True,
        random_state=Config.RANDOM_STATE
    ),
    'neural_network': MLPClassifier(
        hidden_layer_sizes=(100, 50),
        max_iter=1000,
        random_state=Config.RANDOM_STATE
    ),
    'knn': KNeighborsClassifier(
        n_neighbors=5,
        n_jobs=-1
    )
}

# Hyperparameter grids for tuning
param_grids = {
    'logistic_regression': {
        'C': [0.001, 0.01, 0.1, 1, 10],
        'penalty': ['l2'],
        'solver': ['lbfgs']
    },
    'random_forest': {
        'n_estimators': [50, 100, 200],
        'max_depth': [10, 20, None],
        'min_samples_split': [2, 5],
        'min_samples_leaf': [1, 2]
    },
    'gradient_boosting': {
        'n_estimators': [50, 100],
        'learning_rate': [0.01, 0.1],
        'max_depth': [3, 5]
    },
    'svm': {
        'C': [0.1, 1, 10],
        'gamma': ['scale', 'auto']
    },
    'neural_network': {
        'hidden_layer_sizes': [(50,), (100,), (100, 50)],
        'alpha': [0.0001, 0.001]
    },
    'knn': {
        'n_neighbors': [3, 5, 7],
        'weights': ['uniform', 'distance']
    }
}

print(f"‚úÖ Defined {len(models)} models for training")

# %% [markdown]
# ## 10. Model Training Function

# %%
def train_model(model_name, model, X_train, y_train, X_val, y_val, 
                use_hyperparameter_tuning=False):
    """Train a single model with optional hyperparameter tuning"""
    
    print(f"\n{'='*50}")
    print(f"Training {model_name}...")
    start_time = datetime.now()
    
    try:
        # Hyperparameter tuning if requested
        if use_hyperparameter_tuning and model_name in param_grids:
            from sklearn.model_selection import GridSearchCV
            
            print("  Performing hyperparameter tuning...")
            grid_search = GridSearchCV(
                model,
                param_grids[model_name],
                cv=3,
                scoring='f1_weighted',
                n_jobs=-1
            )
            grid_search.fit(X_train, y_train)
            
            best_model = grid_search.best_estimator_
            print(f"  Best parameters: {grid_search.best_params_}")
        else:
            # Train with default parameters
            best_model = model
            best_model.fit(X_train, y_train)
        
        # Make predictions
        y_val_pred = best_model.predict(X_val)
        
        # Calculate metrics
        metrics = {
            'accuracy': accuracy_score(y_val, y_val_pred),
            'precision': precision_score(y_val, y_val_pred, average='weighted'),
            'recall': recall_score(y_val, y_val_pred, average='weighted'),
            'f1': f1_score(y_val, y_val_pred, average='weighted')
        }
        
        # Training time
        training_time = (datetime.now() - start_time).total_seconds()
        
        print(f"  ‚úÖ Training completed in {training_time:.1f}s")
        print(f"  Validation Accuracy: {metrics['accuracy']:.3f}")
        print(f"  Validation F1-Score: {metrics['f1']:.3f}")
        
        return {
            'model': best_model,
            'metrics': metrics,
            'predictions': y_val_pred,
            'training_time': training_time
        }
        
    except Exception as e:
        print(f"  ‚ùå Error training {model_name}: {str(e)}")
        return None

# %% [markdown]
# ## 11. Train All Models

# %%
# Store results
results = {}
training_times = []

# Train each model
for model_name in Config.MODEL_KEYS:
    if model_name not in models:
        print(f"‚ö†Ô∏è  Model {model_name} not found, skipping...")
        continue
    
    result = train_model(
        model_name,
        models[model_name],
        X_train_balanced,
        y_train_balanced,
        X_val_scaled,
        y_val,
        use_hyperparameter_tuning=Config.USE_HYPERPARAMETER_TUNING
    )
    
    if result:
        results[model_name] = result
        training_times.append({
            'model': model_name,
            'time': result['training_time']
        })
    
    # Clear memory
    gc.collect()

monitor.report("Model Training")

# %% [markdown]
# ## 12. Model Comparison

# %%
# Create comparison dataframe
comparison_data = []
for model_name, result in results.items():
    comparison_data.append({
        'Model': model_name.replace('_', ' ').title(),
        'Accuracy': result['metrics']['accuracy'],
        'Precision': result['metrics']['precision'],
        'Recall': result['metrics']['recall'],
        'F1-Score': result['metrics']['f1'],
        'Training Time (s)': result['training_time']
    })

comparison_df = pd.DataFrame(comparison_data)
comparison_df = comparison_df.sort_values('F1-Score', ascending=False)

print("\nüìä Model Performance Comparison:")
print(comparison_df.to_string(index=False, float_format='%.3f'))

# Find best model
best_model_name = comparison_df.iloc[0]['Model'].lower().replace(' ', '_')
best_model_result = results[best_model_name]

print(f"\nüèÜ Best Model: {comparison_df.iloc[0]['Model']}")
print(f"   F1-Score: {comparison_df.iloc[0]['F1-Score']:.3f}")

# %% [markdown]
# ## 13. Visualize Model Comparison

# %%
# Create interactive comparison plot
fig = go.Figure()

# Add bars for each metric
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']

for i, metric in enumerate(metrics):
    fig.add_trace(go.Bar(
        name=metric,
        x=comparison_df['Model'],
        y=comparison_df[metric],
        marker_color=colors[i],
        text=comparison_df[metric].round(3),
        textposition='auto'
    ))

fig.update_layout(
    title='Model Performance Comparison',
    xaxis_title='Model',
    yaxis_title='Score',
    barmode='group',
    height=500,
    showlegend=True
)

fig.show()

# Training time comparison
fig_time = go.Figure(data=[
    go.Bar(
        x=comparison_df['Model'],
        y=comparison_df['Training Time (s)'],
        marker_color='lightcoral',
        text=comparison_df['Training Time (s)'].round(1),
        textposition='auto'
    )
])

fig_time.update_layout(
    title='Model Training Time Comparison',
    xaxis_title='Model',
    yaxis_title='Training Time (seconds)',
    height=400
)

fig_time.show()

# %% [markdown]
# ## 14. Evaluate Best Model on Test Set

# %%
# Get best model
best_model = best_model_result['model']

# Make predictions on test set
y_test_pred = best_model.predict(X_test_scaled)

# Calculate comprehensive metrics
test_metrics = {
    'accuracy': accuracy_score(y_test, y_test_pred),
    'precision_weighted': precision_score(y_test, y_test_pred, average='weighted'),
    'recall_weighted': recall_score(y_test, y_test_pred, average='weighted'),
    'f1_weighted': f1_score(y_test, y_test_pred, average='weighted'),
    'precision_macro': precision_score(y_test, y_test_pred, average='macro'),
    'recall_macro': recall_score(y_test, y_test_pred, average='macro'),
    'f1_macro': f1_score(y_test, y_test_pred, average='macro')
}

print("üéØ Test Set Performance:")
print(f"   Accuracy: {test_metrics['accuracy']:.3f}")
print(f"   F1-Score (weighted): {test_metrics['f1_weighted']:.3f}")
print(f"   F1-Score (macro): {test_metrics['f1_macro']:.3f}")

# Classification report
print("\nüìã Detailed Classification Report:")
print(classification_report(y_test, y_test_pred, 
                          target_names=class_names,
                          digits=3))

# %% [markdown]
# ## 15. Confusion Matrix

# %%
# Calculate confusion matrix
cm = confusion_matrix(y_test, y_test_pred)

# Create interactive heatmap
fig = go.Figure(data=go.Heatmap(
    z=cm,
    x=class_names,
    y=class_names,
    colorscale='Blues',
    text=cm,
    texttemplate='%{text}',
    textfont={"size": 16}
))

fig.update_layout(
    title=f'Confusion Matrix - {best_model_name.replace("_", " ").title()}',
    xaxis_title='Predicted',
    yaxis_title='Actual',
    height=500,
    width=600
)

fig.show()

# Calculate per-class metrics
print("\nüìä Per-Class Performance:")
for i, class_name in enumerate(class_names):
    true_positives = cm[i, i]
    false_positives = cm[:, i].sum() - true_positives
    false_negatives = cm[i, :].sum() - true_positives
    true_negatives = cm.sum() - true_positives - false_positives - false_negatives
    
    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    print(f"   {class_name}: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}")

# %% [markdown]
# ## 16. Clinical Performance Metrics

# %%
# Calculate clinical metrics
clinical_metrics = {}

# MI (Myocardial Infarction) metrics - Class 1
mi_true = (y_test == 1)
mi_pred = (y_test_pred == 1)
clinical_metrics['MI_sensitivity'] = recall_score(mi_true, mi_pred)
clinical_metrics['MI_specificity'] = recall_score(~mi_true, ~mi_pred)
clinical_metrics['MI_ppv'] = precision_score(mi_true, mi_pred, zero_division=0)
clinical_metrics['MI_npv'] = precision_score(~mi_true, ~mi_pred, zero_division=0)

# NORM (Normal) metrics - Class 0
norm_true = (y_test == 0)
norm_pred = (y_test_pred == 0)
clinical_metrics['NORM_sensitivity'] = recall_score(norm_true, norm_pred)
clinical_metrics['NORM_specificity'] = recall_score(~norm_true, ~norm_pred)

# Any abnormality detection
abnormal_true = (y_test != 0)
abnormal_pred = (y_test_pred != 0)
clinical_metrics['Abnormal_sensitivity'] = recall_score(abnormal_true, abnormal_pred)
clinical_metrics['Abnormal_specificity'] = recall_score(~abnormal_true, ~abnormal_pred)

print("üè• Clinical Performance Metrics:")
print(f"   MI Detection:")
print(f"      Sensitivity: {clinical_metrics['MI_sensitivity']:.3f}")
print(f"      Specificity: {clinical_metrics['MI_specificity']:.3f}")
print(f"      PPV: {clinical_metrics['MI_ppv']:.3f}")
print(f"      NPV: {clinical_metrics['MI_npv']:.3f}")
print(f"\n   Normal ECG Detection:")
print(f"      Sensitivity: {clinical_metrics['NORM_sensitivity']:.3f}")
print(f"      Specificity: {clinical_metrics['NORM_specificity']:.3f}")
print(f"\n   Any Abnormality Detection:")
print(f"      Sensitivity: {clinical_metrics['Abnormal_sensitivity']:.3f}")
print(f"      Specificity: {clinical_metrics['Abnormal_specificity']:.3f}")

# %% [markdown]
# ## 17. Feature Importance Analysis

# %%
# Get feature importance (if available)
if hasattr(best_model, 'feature_importances_'):
    feature_importance = best_model.feature_importances_
elif hasattr(best_model, 'coef_'):
    feature_importance = np.abs(best_model.coef_).mean(axis=0)
else:
    print("‚ö†Ô∏è  Feature importance not available for this model type")
    feature_importance = None

if feature_importance is not None:
    # Get top 20 features
    top_indices = np.argsort(feature_importance)[-20:][::-1]
    top_features = [(feature_names[i], feature_importance[i]) for i in top_indices]
    
    # Create visualization
    fig = go.Figure(data=[
        go.Bar(
            x=[f[1] for f in top_features],
            y=[f[0] for f in top_features],
            orientation='h',
            marker_color='lightgreen'
        )
    ])
    
    fig.update_layout(
        title='Top 20 Most Important Features',
        xaxis_title='Importance Score',
        yaxis_title='Feature',
        height=600,
        margin=dict(l=200)
    )
    
    fig.show()
    
    # Print top 10 features
    print("\nüèÜ Top 10 Most Important Features:")
    for i, (feat_name, importance) in enumerate(top_features[:10]):
        print(f"   {i+1:2d}. {feat_name:<40} {importance:.4f}")

# %% [markdown]
# ## 18. Cross-Validation Performance

# %%
if Config.USE_CROSS_VALIDATION:
    print("Performing cross-validation on best model...")
    
    # Perform stratified k-fold cross-validation
    cv_scores = cross_val_score(
        best_model,
        X_train_balanced,
        y_train_balanced,
        cv=StratifiedKFold(n_splits=Config.CV_FOLDS, shuffle=True, random_state=Config.RANDOM_STATE),
        scoring='f1_weighted',
        n_jobs=-1
    )
    
    print(f"\nüìä Cross-Validation Results ({Config.CV_FOLDS}-fold):")
    print(f"   Mean F1-Score: {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}")
    print(f"   Min F1-Score: {cv_scores.min():.3f}")
    print(f"   Max F1-Score: {cv_scores.max():.3f}")
    
    # Visualize CV scores
    fig = go.Figure(data=[
        go.Box(
            y=cv_scores,
            name='CV F1-Scores',
            marker_color='lightblue'
        )
    ])
    
    fig.add_hline(
        y=cv_scores.mean(),
        line_dash="dash",
        line_color="red",
        annotation_text=f"Mean: {cv_scores.mean():.3f}"
    )
    
    fig.update_layout(
        title='Cross-Validation Performance Distribution',
        yaxis_title='F1-Score',
        height=400,
        showlegend=False
    )
    
    fig.show()

# %% [markdown]
# ## 19. Save Trained Models

# %%
# Save best model
best_model_path = Config.MODEL_OUTPUT_DIR / f'{best_model_name}_model.pkl'
joblib.dump(best_model, best_model_path)
print(f"‚úÖ Best model saved to: {best_model_path}")

# Save all results
results_data = {
    'best_model_name': best_model_name,
    'test_metrics': test_metrics,
    'clinical_metrics': clinical_metrics,
    'confusion_matrix': cm,
    'comparison_df': comparison_df,
    'feature_names': feature_names,
    'training_timestamp': datetime.now().isoformat()
}

results_path = Config.MODEL_OUTPUT_DIR / 'phase4_results.pkl'
with open(results_path, 'wb') as f:
    pickle.dump(results_data, f)
print(f"‚úÖ Results saved to: {results_path}")

# Save model metadata
metadata = {
    'model_type': best_model_name,
    'accuracy': test_metrics['accuracy'],
    'f1_score': test_metrics['f1_weighted'],
    'training_samples': len(X_train_balanced),
    'features': len(feature_names),
    'classes': class_names,
    'training_date': datetime.now().isoformat(),
    'phase3_source': str(Config.PHASE3_RESULTS),
    'config': {
        'test_size': Config.TEST_SIZE,
        'val_size': Config.VAL_SIZE,
        'use_smote': Config.USE_SMOTE,
        'use_hyperparameter_tuning': Config.USE_HYPERPARAMETER_TUNING
    }
}

metadata_path = Config.MODEL_OUTPUT_DIR / 'model_metadata.json'
import json
with open(metadata_path, 'w') as f:
    json.dump(metadata, f, indent=2)
print(f"‚úÖ Metadata saved to: {metadata_path}")

monitor.report("Model Saving")

# %% [markdown]
# ## 20. Generate Summary Report

# %%
# Generate comprehensive summary
summary = f"""
# Phase 4: Model Training Summary Report
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Dataset Information
- Total samples: {len(X_features):,}
- Training samples: {len(X_train_balanced):,} (after SMOTE)
- Validation samples: {len(X_val):,}
- Test samples: {len(X_test):,}
- Number of features: {len(feature_names)}
- Number of classes: {len(class_names)}

## Best Model Performance
- Model: {best_model_name.replace('_', ' ').title()}
- Test Accuracy: {test_metrics['accuracy']:.3f}
- Test F1-Score: {test_metrics['f1_weighted']:.3f}
- Training Time: {best_model_result['training_time']:.1f} seconds

## Clinical Performance
- MI Detection Sensitivity: {clinical_metrics['MI_sensitivity']:.3f}
- MI Detection Specificity: {clinical_metrics['MI_specificity']:.3f}
- Normal ECG Specificity: {clinical_metrics['NORM_specificity']:.3f}
- Any Abnormality Sensitivity: {clinical_metrics['Abnormal_sensitivity']:.3f}

## Model Comparison Results
{comparison_df.to_string(index=False, float_format='%.3f')}

## Per-Class Performance
"""

for i, class_name in enumerate(class_names):
    report_dict = classification_report(y_test, y_test_pred, output_dict=True)
    class_metrics = report_dict[str(i)]
    summary += f"- {class_name}: Precision={class_metrics['precision']:.3f}, "
    summary += f"Recall={class_metrics['recall']:.3f}, F1={class_metrics['f1-score']:.3f}\n"

if Config.USE_CROSS_VALIDATION:
    summary += f"\n## Cross-Validation Results\n"
    summary += f"- {Config.CV_FOLDS}-Fold CV F1-Score: {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}\n"

summary += f"\n## Configuration\n"
summary += f"- SMOTE Applied: {Config.USE_SMOTE}\n"
summary += f"- Hyperparameter Tuning: {Config.USE_HYPERPARAMETER_TUNING}\n"
summary += f"- Test Size: {Config.TEST_SIZE}\n"
summary += f"- Validation Size: {Config.VAL_SIZE}\n"

# Save summary
summary_path = Config.MODEL_OUTPUT_DIR / 'training_summary.txt'
with open(summary_path, 'w') as f:
    f.write(summary)
print(f"‚úÖ Summary report saved to: {summary_path}")

# Display summary
print("\n" + "="*60)
print(summary)
print("="*60)

# %% [markdown]
# ## 21. Final Performance Summary

# %%
# Create final performance visualization
fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=('Test Set Metrics', 'Clinical Metrics', 
                    'Model Comparison', 'Confusion Matrix'),
    specs=[[{'type': 'bar'}, {'type': 'bar'}],
           [{'type': 'bar'}, {'type': 'heatmap'}]],
    vertical_spacing=0.15,
    horizontal_spacing=0.1
)

# Test set metrics
test_metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
test_metric_values = [
    test_metrics['accuracy'],
    test_metrics['precision_weighted'],
    test_metrics['recall_weighted'],
    test_metrics['f1_weighted']
]

fig.add_trace(
    go.Bar(x=test_metric_names, y=test_metric_values, 
           marker_color='lightblue', text=np.round(test_metric_values, 3),
           textposition='auto'),
    row=1, col=1
)

# Clinical metrics
clinical_names = ['MI Sens', 'MI Spec', 'NORM Spec', 'Abnormal Sens']
clinical_values = [
    clinical_metrics['MI_sensitivity'],
    clinical_metrics['MI_specificity'],
    clinical_metrics['NORM_specificity'],
    clinical_metrics['Abnormal_sensitivity']
]

fig.add_trace(
    go.Bar(x=clinical_names, y=clinical_values,
           marker_color='lightgreen', text=np.round(clinical_values, 3),
           textposition='auto'),
    row=1, col=2
)

# Model comparison (top 3)
top_models = comparison_df.head(3)
fig.add_trace(
    go.Bar(x=top_models['Model'], y=top_models['F1-Score'],
           marker_color='lightcoral', text=top_models['F1-Score'].round(3),
           textposition='auto'),
    row=2, col=1
)

# Confusion matrix
fig.add_trace(
    go.Heatmap(z=cm, x=class_names, y=class_names,
               colorscale='Blues', text=cm, texttemplate='%{text}'),
    row=2, col=2
)

fig.update_layout(
    title_text=f"Phase 4 Complete Results - Best Model: {best_model_name.replace('_', ' ').title()}",
    height=800,
    showlegend=False
)

fig.update_xaxes(title_text="Metric", row=1, col=1)
fig.update_xaxes(title_text="Metric", row=1, col=2)
fig.update_xaxes(title_text="Model", row=2, col=1)
fig.update_xaxes(title_text="Predicted", row=2, col=2)

fig.update_yaxes(title_text="Score", row=1, col=1)
fig.update_yaxes(title_text="Score", row=1, col=2)
fig.update_yaxes(title_text="F1-Score", row=2, col=1)
fig.update_yaxes(title_text="Actual", row=2, col=2)

fig.show()

# Save visualization
if Config.SAVE_PLOTS:
    fig.write_html(Config.VISUALIZATION_DIR / 'phase4_summary.html')
    print(f"‚úÖ Summary visualization saved")

# %% [markdown]
# ## 22. Next Steps

# %%
print("\nüéØ Phase 4 Completed Successfully!")
print("\nüìù Next Steps:")
print("1. ‚úÖ Models trained and evaluated")
print("2. ‚úÖ Best model selected and saved")
print("3. ‚úÖ Performance metrics calculated")
print("4. ‚úÖ Clinical metrics evaluated")
print("\nüöÄ Ready for Phase 5: Deployment")
print("\nSaved files:")
print(f"- Best model: {best_model_path}")
print(f"- Scaler: {scaler_path}")
print(f"- Results: {results_path}")
print(f"- Summary: {summary_path}")

# Final memory report
final_memory = monitor.get_memory_usage()
print(f"\nüíæ Total memory used: {final_memory:.1f} MB")
print(f"‚è±Ô∏è  Total execution time: {(datetime.now() - monitor.start_time).total_seconds():.1f} seconds")